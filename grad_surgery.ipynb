{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5955, -0.1214,  0.7084],\n",
      "        [-0.0214, -0.3130, -0.1566],\n",
      "        [ 0.5900,  0.1193, -0.7024],\n",
      "        [ 0.4459,  0.0869, -0.5328]])\n",
      "tensor([ 0.1312, -0.3516, -0.1312, -0.1028])\n",
      "--------------------------------------------------------------------------------\n",
      "tensor([[ 0.1543,  0.1705, -0.1015],\n",
      "        [-0.4761, -0.3491,  0.4177]])\n",
      "tensor([ 0.1266, -0.1860])\n",
      "tensor([[-0.0333,  0.0969],\n",
      "        [-0.1064,  0.0918],\n",
      "        [-0.0381, -0.0076],\n",
      "        [-0.0717,  0.0192]])\n",
      "tensor([-0.1726, -0.2270, -0.0211, -0.0894])\n",
      "tensor([[ 0.1493,  0.0015],\n",
      "        [-0.3189,  0.1969],\n",
      "        [ 0.0228, -0.0440],\n",
      "        [-0.1457,  0.0503]])\n",
      "tensor([ 0.1246, -0.5640,  0.0848, -0.1987])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pdb\n",
    "import numpy as np\n",
    "import copy\n",
    "import random\n",
    "\n",
    "\n",
    "class PCGrad:\n",
    "    def __init__(self, optimizer, reduction=\"mean\"):\n",
    "        self._optim, self._reduction = optimizer, reduction\n",
    "        return\n",
    "\n",
    "    @property\n",
    "    def optimizer(self):\n",
    "        return self._optim\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"\n",
    "        clear the gradient of the parameters\n",
    "        \"\"\"\n",
    "\n",
    "        return self._optim.zero_grad(set_to_none=True)\n",
    "\n",
    "    def step(self):\n",
    "        \"\"\"\n",
    "        update the parameters with the gradient\n",
    "        \"\"\"\n",
    "\n",
    "        return self._optim.step()\n",
    "\n",
    "    def pc_backward(self, objectives):\n",
    "        \"\"\"\n",
    "        calculate the gradient of the parameters\n",
    "\n",
    "        input:\n",
    "        - objectives: a list of objectives\n",
    "        \"\"\"\n",
    "\n",
    "        grads, shapes, has_grads = self._pack_grad(objectives)\n",
    "        pc_grad = self._project_conflicting(grads, has_grads)\n",
    "        pc_grad = self._unflatten_grad(pc_grad, shapes[0])\n",
    "        self._set_grad(pc_grad)\n",
    "        return\n",
    "\n",
    "    def _project_conflicting(self, grads, has_grads, shapes=None):\n",
    "        shared = torch.stack(has_grads).prod(0).bool()\n",
    "        pc_grad, num_task = copy.deepcopy(grads), len(grads)\n",
    "        for g_i in pc_grad:\n",
    "            random.shuffle(grads)\n",
    "            for g_j in grads:\n",
    "                g_i_g_j = torch.dot(g_i, g_j)\n",
    "                if g_i_g_j < 0:\n",
    "                    g_i -= (g_i_g_j) * g_j / (g_j.norm() ** 2)\n",
    "        merged_grad = torch.zeros_like(grads[0]).to(grads[0].device)\n",
    "        if self._reduction:\n",
    "            merged_grad[shared] = torch.stack([g[shared] for g in pc_grad]).mean(dim=0)\n",
    "        elif self._reduction == \"sum\":\n",
    "            merged_grad[shared] = torch.stack([g[shared] for g in pc_grad]).sum(dim=0)\n",
    "        else:\n",
    "            exit(\"invalid reduction method\")\n",
    "\n",
    "        merged_grad[~shared] = torch.stack([g[~shared] for g in pc_grad]).sum(dim=0)\n",
    "        return merged_grad\n",
    "\n",
    "    def _set_grad(self, grads):\n",
    "        \"\"\"\n",
    "        set the modified gradients to the network\n",
    "        \"\"\"\n",
    "\n",
    "        idx = 0\n",
    "        for group in self._optim.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                # if p.grad is None: continue\n",
    "                p.grad = grads[idx]\n",
    "                idx += 1\n",
    "        return\n",
    "\n",
    "    def _pack_grad(self, objectives):\n",
    "        \"\"\"\n",
    "        pack the gradient of the parameters of the network for each objective\n",
    "\n",
    "        output:\n",
    "        - grad: a list of the gradient of the parameters\n",
    "        - shape: a list of the shape of the parameters\n",
    "        - has_grad: a list of mask represent whether the parameter has gradient\n",
    "        \"\"\"\n",
    "\n",
    "        grads, shapes, has_grads = [], [], []\n",
    "        for obj in objectives:\n",
    "            self._optim.zero_grad(set_to_none=True)\n",
    "            obj.backward(retain_graph=True)\n",
    "            grad, shape, has_grad = self._retrieve_grad()\n",
    "            grads.append(self._flatten_grad(grad, shape))\n",
    "            has_grads.append(self._flatten_grad(has_grad, shape))\n",
    "            shapes.append(shape)\n",
    "        return grads, shapes, has_grads\n",
    "\n",
    "    def _unflatten_grad(self, grads, shapes):\n",
    "        unflatten_grad, idx = [], 0\n",
    "        for shape in shapes:\n",
    "            length = np.prod(shape)\n",
    "            unflatten_grad.append(grads[idx : idx + length].view(shape).clone())\n",
    "            idx += length\n",
    "        return unflatten_grad\n",
    "\n",
    "    def _flatten_grad(self, grads, shapes):\n",
    "        flatten_grad = torch.cat([g.flatten() for g in grads])\n",
    "        return flatten_grad\n",
    "\n",
    "    def _retrieve_grad(self):\n",
    "        \"\"\"\n",
    "        get the gradient of the parameters of the network with specific\n",
    "        objective\n",
    "\n",
    "        output:\n",
    "        - grad: a list of the gradient of the parameters\n",
    "        - shape: a list of the shape of the parameters\n",
    "        - has_grad: a list of mask represent whether the parameter has gradient\n",
    "        \"\"\"\n",
    "\n",
    "        grad, shape, has_grad = [], [], []\n",
    "        for group in self._optim.param_groups:\n",
    "            for p in group[\"params\"]:\n",
    "                # if p.grad is None: continue\n",
    "                # tackle the multi-head scenario\n",
    "                if p.grad is None:\n",
    "                    shape.append(p.shape)\n",
    "                    grad.append(torch.zeros_like(p).to(p.device))\n",
    "                    has_grad.append(torch.zeros_like(p).to(p.device))\n",
    "                    continue\n",
    "                shape.append(p.grad.shape)\n",
    "                grad.append(p.grad.clone())\n",
    "                has_grad.append(torch.ones_like(p).to(p.device))\n",
    "        return grad, shape, has_grad\n",
    "\n",
    "\n",
    "class TestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._linear = nn.Linear(3, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._linear(x)\n",
    "\n",
    "\n",
    "class MultiHeadTestNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._linear = nn.Linear(3, 2)\n",
    "        self._head1 = nn.Linear(2, 4)\n",
    "        self._head2 = nn.Linear(2, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feat = self._linear(x)\n",
    "        return self._head1(feat), self._head2(feat)\n",
    "\n",
    "# fully shared network test\n",
    "torch.manual_seed(4)\n",
    "x, y = torch.randn(2, 3), torch.randn(2, 4)\n",
    "net = TestNet()\n",
    "y_pred = net(x)\n",
    "pc_adam = PCGrad(optim.Adam(net.parameters()))\n",
    "pc_adam.zero_grad()\n",
    "loss1_fn, loss2_fn = nn.L1Loss(), nn.MSELoss()\n",
    "loss1, loss2 = loss1_fn(y_pred, y), loss2_fn(y_pred, y)\n",
    "pc_adam.pc_backward([loss1, loss2])\n",
    "# (loss1 + loss2).backward()\n",
    "# tensor([[-1.1910, -0.2429,  1.4168],\n",
    "#         [-0.0428, -0.6260, -0.3132],\n",
    "#         [ 1.1799,  0.2386, -1.4048],\n",
    "#         [ 0.8918,  0.1739, -1.0656]])\n",
    "# tensor([ 0.2624, -0.7032, -0.2623, -0.2057])\n",
    "for p in net.parameters():\n",
    "    print(p.grad)\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# seperated shared network test\n",
    "torch.manual_seed(4)\n",
    "x, y = torch.randn(2, 3), torch.randn(2, 4)\n",
    "net = MultiHeadTestNet()\n",
    "y_pred_1, y_pred_2 = net(x)\n",
    "pc_adam = PCGrad(optim.Adam(net.parameters()))\n",
    "pc_adam.zero_grad()\n",
    "loss1_fn, loss2_fn = nn.MSELoss(), nn.MSELoss()\n",
    "loss1, loss2 = loss1_fn(y_pred_1, y), loss2_fn(y_pred_2, y)\n",
    "# (loss1 + loss2).backward()\n",
    "# tensor([[ 0.3086,  0.3411, -0.2030],\n",
    "#         [-0.9521, -0.6981,  0.8353]])\n",
    "# tensor([ 0.2531, -0.3721])\n",
    "# tensor([[-0.0333,  0.0969],\n",
    "#         [-0.1064,  0.0918],\n",
    "#         [-0.0381, -0.0076],\n",
    "#         [-0.0717,  0.0192]])\n",
    "# tensor([-0.1726, -0.2270, -0.0211, -0.0894])\n",
    "# tensor([[ 0.1493,  0.0015],\n",
    "#         [-0.3189,  0.1969],\n",
    "#         [ 0.0228, -0.0440],\n",
    "#         [-0.1457,  0.0503]])\n",
    "# tensor([ 0.1246, -0.5640,  0.0848, -0.1987])\n",
    "pc_adam.pc_backward([loss1, loss2])\n",
    "for p in net.parameters():\n",
    "    print(p.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2], [3, 4]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randint(0, 2, (2, 3))\n",
    "y = torch.randint(0, 2, (2, 3))\n",
    "x, (x + x).sum()\n",
    "\n",
    "a = [[1, 2]]\n",
    "a.append([3, 4])\n",
    "a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 4]) torch.Size([15])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.7895)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "logits = torch.randn(3, 5, 4)\n",
    "labels = torch.randint(0, 4, (3, 5))\n",
    "loss = F.cross_entropy(logits.view(-1, 4), labels.view(-1))\n",
    "print(logits.view(-1, 4).shape, labels.view(-1).shape)\n",
    "loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
